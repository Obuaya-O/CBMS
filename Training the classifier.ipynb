{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifier of predicting primary endpoint type in the EUCT-NS dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the optimal word embedding technique, classifier and parameters as determined by the previous step of LOOCV to train the classifier. This was Complement Naive Bayes with TF-IDF embeddings.\n",
    "Train/Test Split: We divided the EUCT-NS dataset into an 80/20 train/test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import model \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, classification_report, roc_curve, precision_recall_curve\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Import feature‚Äêextraction tools (e.g., TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euct_ns = pd.read_csv('c:\\\\Users\\\\s2421127\\\\Documents\\\\NLP Project\\\\ObuayaO\\\\NLP project\\\\Chapter 3\\\\euct_ns.csv', encoding='unicode_escape')\n",
    "\n",
    "euct_ns['concat_corpus'] = euct_ns['Title']+ \" \" + euct_ns['Objective'] + \" \" + euct_ns['pr_endpoint'] + \" \" + euct_ns['endpoint_description']\n",
    "\n",
    "euct_ns['concat_corpus'] = euct_ns['concat_corpus'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the embeddings for the corpus For TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html For GloVe: https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/DOCUMENT_POOL_EMBEDDINGS.md For SciBERT: https://github.com/allenai/scibert?tab=readme-ov-file\n",
    "\n",
    "These are the word embeddings I used but there are other options to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = data.data # feature matrix after embedding transformation\n",
    "y = euct_df['manual_label'].values # target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(embedded_transformed_features, \"feature_embeddings.pkl\")\n",
    "# Save the feature matrix after embedding transformation to apply later to the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "mdl = MDL(parameters of mdl, change these to the best parameters found by GridSearchCV in LOOCV, random_state = 42) # Replace Model with your chosen model, e.g., RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "mdl.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = mdl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(mdl, \"model.pkl\")\n",
    "# Save the trained model to a file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "target_names = ['class 0', 'class 1', 'class 2'] # Patient Final Outcome class, Intermediate Outcome Class and Surrogate Outcome Class\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Calculate AUROC score using predicted probabilities\n",
    "auroc_weighted = roc_auc_score(y_test, y_pred_proba, average='weighted', multi_class='ovr')\n",
    "\n",
    "# Compute ROC curve for each class\n",
    "for i in range(mdl.classes_.shape[0]):\n",
    "\tfpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)\n",
    "\tplt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Training ROC Curve Comparison Between Classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute precision-recall curve for each class\n",
    "for i in range(mdl.classes_.shape[0]):\n",
    "\ty_prob_train = mdl.predict_proba(X_train)[:, i]\n",
    "\ty_prob_test = mdl.predict_proba(X_test)[:, i]\n",
    "\n",
    "\tprecision, recall, thresholds = precision_recall_curve(y_train == i, y_prob_train)\n",
    "\tplt.plot(recall, precision, lw=2, label=f'Class {i}')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Training Precision-Recall curve\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# Compute confusion matrix and display it\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=mdl.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model trained on the EUCT-NS dataset to second dataset:\n",
    "Due to data sharing agreement (see LOOCV). We cannot share the NS-HRA dataset. The NS-HRA dataset was formed in a similar way to the EUCT-NS dataset but instead of HTML parsing, it was XML parsing. One could form a comparable second dataset through this approach. The method for the formation of specialised datasets using clinical trial documentation will be shared at a later date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_ds = pd.read_csv(second_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_ds['concat_corpus'] = second_ds['Title']+ \" \" + second_ds['Objective'] + \" \" + second_ds['1ry_endpoint'] \n",
    "second_ds['concat_corpus'] = second_ds['concat_corpus'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_transformed_features = joblib.load(\"feature_embeddings.pkl\")\n",
    "# Load the feature matrix after embedding transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = embedded_transformed_features.transform(second_ds['concat_corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = joblib.load('model.pkl')\n",
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the second dataset\n",
    "model.predict(X2)\n",
    "y_pred = model.predict(X2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_endpoints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
