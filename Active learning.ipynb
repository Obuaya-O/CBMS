{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active learning is a semi-supervised, machine learning approach that utilises human insight in tandem with natural language processing (NLP) algorithms to annotate data (Settles, 2009).\n",
    "\n",
    "Query: We used a predefined query function of least certainty sampling to determine what data was to be labelled in each round of active learning. We determined this first uncertainty by applying the classifier previously trained on the EUCT-NS dataset to the NS-HRA dataset. We labelled seven cases for each round of active learning\n",
    "Annotate: The command: “Enter label for this case (0, 1, 2):” was answered by the first author and each queried case was labelled.\n",
    "Append: The newly-labelled examples were removed from the unlabelled dataset (U) and the model was retrained on the labelled cases (L) and applied to U to acquire new uncertainty samples. The active learning loop was repeated until the stop criterion was achieved: twenty loops of active learning (140 cases labelled) or a consistent (across five loops) weighted-recall of ≥ 0.7.\n",
    "\n",
    "Performance metrics\n",
    "To better evaluate the performance of our multiclass task, we utilised weighted accuracy, weighted presion, recall and f1-score, area under the receiver operator curve (AUROC) and the precision-recall curve. \n",
    "\n",
    "In regard to the interpretation of the metrics: AUROC, weighted accuracy, precision, recall and f1-score are valued between 0 and 1, with values nearer to 1 indicating better performance (Kuo et al., 2020). \n",
    "\n",
    "For our study, it was determined that recall would be the most important metric as our aim was to identify all cases where a clinical trial protocol depicts a surrogate outcome as the primary endpoint, even if that means occasionally misclassifying some instances. Whilst this approach may increase false positives, it ensures that we capture all potential primary surrogate endpoint usage, which is essential to understand the broader implications of poor reporting of surrogate outcomes in clinical trials. A binary classification of “surrogate vs. non-surrogate” would not have provided the comprehensive view necessary to distinguish between primary endpoints which are not surrogate outcomes but also do not directly contribute to patient health in a final manner - intermediate outcomes (Griffiths et al., 2022). The trade-off between false positives and recall was carefully considered, acknowledging that the cost of misclassifications is outweighed by the importance of minimising false negatives in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages and libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import model \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Import feature‐extraction tools (e.g., TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the pretrained model to the unlabelled dataset:\n",
    "We used the model trained on the EUCT-NS dataset to generate probability scores on our second dataset (NS-HRA dataset), then selected the lowest‐confidence (highest‐uncertainty)predictions as the initial queries for our active‐learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unlabelled second dataset\n",
    "X2_unlabeled = X2_df.copy()\n",
    "\n",
    "# Obtain probabilities and calculate uncertainty scores\n",
    "probs = model.predict_proba(X2_unlabeled)\n",
    "uncertainty_scores = np.abs(probs - 0.5).min(axis=1)  # Least confidence sampling\n",
    "\n",
    "# Select the top N most uncertain samples\n",
    "n_samples = x # Define how many samples you want to select, we initially selected 7 (~1% of the dataset). We did an alternate approach of selecting \n",
    "# 1 sample per active learning iteration which is more efficient for a small dataset.\n",
    "most_uncertain_indices = uncertainty_scores.argsort()[-n_samples:]\n",
    "\n",
    "# Extract the corresponding Unique_IDs and preprocessed concatenated text. The unique IDs are used to identify the samples in the original dataset that \n",
    "# have not had the text preprocessed. We used this original dataset, the health outcome framework from Manyara et al., 2022 and a systematic search in the Core Outcome Measures in Effectiveness Trials \n",
    "# (COMET) database to determine primary endpoint type.\n",
    "X_initial_raw = ns_hra.iloc[most_uncertain_indices]\n",
    "\n",
    "def manually_label_samples(selected_rows):\n",
    "    labels = []\n",
    "    for idx, row in selected_rows.iterrows():\n",
    "        print(f\"Unique_ID: {row['Unique_ID']}\")\n",
    "        print(f\"Preprocessed Text: {row['concat_corpus']}\\n\")\n",
    "        label = input(\"Enter label for this sample (e.g., 0, 1, 2): \")\n",
    "        labels.append(int(label))\n",
    "    return labels\n",
    "\n",
    "# Manually label the selected samples\n",
    "y_initial = manually_label_samples(X_initial_raw)\n",
    "X_initial = X_initial_raw['concat_corpus'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure X_train is always a list before extending\n",
    "if 'X_train' not in locals() or not isinstance(X_train, list):\n",
    "    X_train = list(X_initial)  # Convert to list explicitly\n",
    "    y_train = np.array(y_initial)\n",
    "else:\n",
    "    X_train.extend(X_initial)\n",
    "    y_train = np.hstack([y_train, y_initial])\n",
    "\n",
    "# Remove labeled samples from unlabeled pool\n",
    "X2_unlabeled_reset = X2_unlabeled.drop(index=most_uncertain_indices).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train the new classifier:\n",
    "We performed another grid search (see LOOCV) to determine if the best classifier was still the same as the one we had previously selected. \n",
    "We found that the best classifier was still the same, so we used it to train the model on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    # Add your feature extraction step here, e.g., TfidfVectorizer),\n",
    "    mdl(best_hyperparameters as determined by grid search)\n",
    ")\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X2_unlabeled_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We applied cross-validation to the training set to obtain predictions for the training set.\n",
    "y_pred_cv = cross_val_predict(pipeline, X_train, y_train, cv=3) # cv increased the larger our sample size became. We used 3 folds for the initial sample size of 7 and increased to \n",
    "#5 folds on iteration 4 and 10 folds on iteration 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = classification_report(y_train, y_pred_cv, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the active learning loop with the new model and updated dataset\n",
    "X2_unlabeled = X2_unlabeled_reset.copy()\n",
    "...\n",
    "# Manually label the selected samples\n",
    "y2_initial = manually_label_samples(X_initial_raw)\n",
    "X2_initial = X_initial_raw['concat_corpus'].tolist() # Change the variable name for each new given iteration to avoid confusion with the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_initial = list(X2_initial)\n",
    "y_train = np.array(y2_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the newly labelled samples to the previous training set for each new iteration until stop criteria is met.\n",
    "X_train = X_train + X2_initial\n",
    "y_train = np.hstack([y_initial, y2_initial])\n",
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove labelled samples from unlabelled pool\n",
    "X2_unlabeled_reset = X2_unlabeled.drop(index=most_uncertain_indices).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X2_unlabeled_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cv = cross_val_predict(pipeline, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter2 = classification_report(y_train, y_pred_cv, output_dict=True)\n",
    "print(iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeated this a total of 20 times (see README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Gather all classification reports\n",
    "iterations = 20\n",
    "classification_reports = [globals()[f'iter{i}'] for i in range(1, iterations+1)]\n",
    "\n",
    "# Step 2: Extract metrics\n",
    "accuracy_list = []\n",
    "f1_score_0 = []\n",
    "f1_score_1 = []\n",
    "f1_score_2 = []\n",
    "\n",
    "for report in classification_reports:\n",
    "    accuracy_list.append(report['accuracy'])\n",
    "    f1_score_0.append(report['0']['f1-score'])\n",
    "    f1_score_1.append(report['1']['f1-score'])\n",
    "    f1_score_2.append(report['2']['f1-score'])\n",
    "\n",
    "iterations = np.arange(1, 21)\n",
    "\n",
    "# Step 3: Plot accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, accuracy_list, marker='o', linestyle='-', color='b', label='Accuracy')\n",
    "plt.axhline(y=#baseline_accuracy color='r', linestyle='--', label='Baseline Accuracy')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Iteration')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Plot F1 scores per class\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(iterations, f1_score_0, marker='o', linestyle='-', color='r', label='F1-score Class 0 (PFO)')\n",
    "plt.plot(iterations, f1_score_1, marker='s', linestyle='--', color='g', label='F1-score Class 1 (IO)')\n",
    "plt.plot(iterations, f1_score_2, marker='^', linestyle='-.', color='m', label='F1-score Class 2 (SO)')\n",
    "plt.axhline(y=#baseline weighted f1_score , color='r', linestyle='--', label='Baseline Weighted F1-score')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('F1-Score per Class vs. Iteration')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional work: Precision recall curve and threshold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pipeline.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target recall threshold\n",
    "desired_recall = 0.70\n",
    "selected_thresholds = {}\n",
    "\n",
    "# For each class (assuming classes 0, 1, 2)\n",
    "classes = [0, 1, 2]\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"\\n--- Class {cls} ---\")\n",
    "    \n",
    "    # Binary ground truth: 1 if current class, 0 otherwise\n",
    "    y_true_bin = (y_train == cls).astype(int)\n",
    "    \n",
    "    # Predicted probability scores for the current class\n",
    "    y_score = y_pred_proba[:, cls]\n",
    "    \n",
    "    # Compute precision-recall pairs\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true_bin, y_score)\n",
    "    \n",
    "    # precision_recall_curve gives (n_thresholds + 1) recall/precision points\n",
    "    # thresholds has shape (n_thresholds,)\n",
    "    \n",
    "    # Find index where recall crosses desired_recall\n",
    "    try:\n",
    "        index = np.where(recall >= desired_recall)[0][0]\n",
    "        \n",
    "        if index >= len(thresholds):\n",
    "            # Edge case: the last recall value doesn't have a corresponding threshold\n",
    "            selected_threshold = thresholds[-1]\n",
    "        else:\n",
    "            selected_threshold = thresholds[index]\n",
    "        \n",
    "        selected_thresholds[cls] = selected_threshold\n",
    "        print(f\"Threshold @ Recall ≥ {desired_recall:.2f}: {selected_threshold:.4f}\")\n",
    "        print(f\"  Recall: {recall[index]:.2f}, Precision: {precision[index]:.2f}\")\n",
    "        \n",
    "    except IndexError:\n",
    "        print(f\"No threshold found for class {cls} reaching recall ≥ {desired_recall}\")\n",
    "        selected_thresholds[cls] = None\n",
    "    \n",
    "    # Plot the Precision-Recall curve\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, marker='.', label=f'Class {cls}')\n",
    "    plt.axvline(x=desired_recall, color='red', linestyle='--', label=f'Target Recall = {desired_recall:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve for Class {cls}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSelected thresholds per class:\")\n",
    "for cls, thresh in selected_thresholds.items():\n",
    "    print(f\"Class {cls}: {thresh}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biasing the model for the surrogate outcome (class 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_class_2 = selected_thresholds[2]\n",
    "\n",
    "# New adjusted predictions\n",
    "adjusted_preds = []\n",
    "\n",
    "for prob in y_pred_proba:\n",
    "    if prob[2] >= threshold_class_2:\n",
    "        adjusted_preds.append(2)  # Force predict class 2\n",
    "    else:\n",
    "        adjusted_preds.append(np.argmax(prob))  # Otherwise, normal prediction\n",
    "\n",
    "adjusted_preds = np.array(adjusted_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, adjusted_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# Store precision and recall at each threshold\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    adjusted_preds = []\n",
    "    \n",
    "    for prob in y_pred_proba:\n",
    "        if prob[2] >= thresh:\n",
    "            adjusted_preds.append(2)\n",
    "        else:\n",
    "            adjusted_preds.append(np.argmax(prob))\n",
    "    \n",
    "    adjusted_preds = np.array(adjusted_preds)\n",
    "    \n",
    "    # Precision and recall class 2\n",
    "    precision = precision_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    recall = recall_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    \n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, precisions, label='Precision', color='blue')\n",
    "plt.plot(thresholds, recalls, label='Recall', color='green')\n",
    "plt.axhline(0.7, color='red', linestyle='--', label='Target Recall = 0.70')\n",
    "plt.axvline(threshold_class_2, color='purple', linestyle='--', label=f'Selected Threshold = {threshold_class_2:.2f}')\n",
    "plt.xlabel('Threshold for Class 2')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs Threshold (Class 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 500)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "\n",
    "for thresh in thresholds:\n",
    "    adjusted_preds = []\n",
    "    \n",
    "    for prob in y_pred_proba:\n",
    "        if prob[2] >= thresh:\n",
    "            adjusted_preds.append(2)\n",
    "        else:\n",
    "            adjusted_preds.append(np.argmax(prob))\n",
    "    \n",
    "    adjusted_preds = np.array(adjusted_preds)\n",
    "    \n",
    "    # Calculate precision, recall, f1 for class 2 only\n",
    "    precision = precision_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    recall = recall_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    f1 = f1_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = thresh\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "\n",
    "print(f\"Best threshold for Class 2 = {best_threshold:.4f}\")\n",
    "print(f\"  F1-score = {best_f1:.4f}\")\n",
    "print(f\"  Precision = {best_precision:.4f}\")\n",
    "print(f\"  Recall = {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    adjusted_preds = []\n",
    "    \n",
    "    for prob in y_pred_proba:\n",
    "        if prob[2] >= thresh:\n",
    "            adjusted_preds.append(2)\n",
    "        else:\n",
    "            adjusted_preds.append(np.argmax(prob))\n",
    "    \n",
    "    adjusted_preds = np.array(adjusted_preds)\n",
    "    \n",
    "    f1 = f1_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Plot F1 vs Threshold\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(thresholds, f1_scores, color='blue')\n",
    "plt.axvline(best_threshold, color='red', linestyle='--', label=f'Best Threshold = {best_threshold:.2f}')\n",
    "plt.xlabel('Threshold for Class 2')\n",
    "plt.ylabel('F1-score (Class 2)')\n",
    "plt.title('F1-Score vs Threshold (Class 2)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_recall = 0.7  \n",
    "thresholds = np.linspace(0, 1, 500)\n",
    "\n",
    "# To store candidates\n",
    "valid_thresholds = []\n",
    "valid_precisions = []\n",
    "valid_recalls = []\n",
    "valid_f1s = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    adjusted_preds = []\n",
    "    \n",
    "    for prob in y_pred_proba:\n",
    "        if prob[2] >= thresh:\n",
    "            adjusted_preds.append(2)\n",
    "        else:\n",
    "            adjusted_preds.append(np.argmax(prob))\n",
    "    \n",
    "    adjusted_preds = np.array(adjusted_preds)\n",
    "    \n",
    "    # Evaluate class 2 only\n",
    "    precision = precision_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    recall = recall_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    f1 = f1_score(y_train, adjusted_preds, labels=[2], average='micro', zero_division=0)\n",
    "    \n",
    "    # Only accept thresholds meeting the recall constraint\n",
    "    if recall >= desired_recall:\n",
    "        valid_thresholds.append(thresh)\n",
    "        valid_precisions.append(precision)\n",
    "        valid_recalls.append(recall)\n",
    "        valid_f1s.append(f1)\n",
    "\n",
    "# Select best precision among valid thresholds\n",
    "if valid_thresholds:\n",
    "    best_idx = np.argmax(valid_precisions)\n",
    "    best_threshold = valid_thresholds[best_idx]\n",
    "    best_precision = valid_precisions[best_idx]\n",
    "    best_recall = valid_recalls[best_idx]\n",
    "    best_f1 = valid_f1s[best_idx]\n",
    "    \n",
    "    print(f\"Best threshold (recall ≥ {desired_recall}): {best_threshold:.4f}\")\n",
    "    print(f\"  Precision: {best_precision:.4f}\")\n",
    "    print(f\"  Recall: {best_recall:.4f}\")\n",
    "    print(f\"  F1-score: {best_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"No threshold found achieving recall ≥ {desired_recall}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if valid_thresholds:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(valid_thresholds, valid_precisions, marker='o', label='Precision')\n",
    "    plt.plot(valid_thresholds, valid_recalls, marker='x', label='Recall')\n",
    "    plt.plot(valid_thresholds, valid_f1s, marker='^', label='F1-score')\n",
    "    plt.axhline(desired_recall, color='red', linestyle='--', label=f'Target Recall = {desired_recall}')\n",
    "    plt.xlabel('Threshold for Class 2')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Valid Thresholds (Recall ≥ Target)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering_endpoints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
